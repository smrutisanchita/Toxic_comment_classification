{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom keras.models import Model, model_from_json\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate,  Dropout\n#from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc\nfrom sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,accuracy_score,classification_report\nfrom sklearn.svm import SVC\nimport os\nimport sys\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport re, string, unicodedata\nfrom collections import defaultdict\n\nimport nltk\nfrom nltk import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer ,PorterStemmer,WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom yellowbrick.text import FreqDistVisualizer\nfrom textblob import Word\n\nfrom keras.preprocessing.text import Tokenizer  \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras import models\nfrom keras import layers\nfrom keras.utils.vis_utils import plot_model\n\nimport matplotlib.pyplot as plt\nimport re\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Dropout, Conv1D, GlobalMaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.utils.vis_utils import plot_model\nfrom sklearn.metrics import roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-26T18:47:11.742686Z","iopub.execute_input":"2021-08-26T18:47:11.743011Z","iopub.status.idle":"2021-08-26T18:47:36.333762Z","shell.execute_reply.started":"2021-08-26T18:47:11.742934Z","shell.execute_reply":"2021-08-26T18:47:36.332616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http://matplotlib.org/examples/color/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:47:36.335537Z","iopub.execute_input":"2021-08-26T18:47:36.335992Z","iopub.status.idle":"2021-08-26T18:47:36.349592Z","shell.execute_reply.started":"2021-08-26T18:47:36.335919Z","shell.execute_reply":"2021-08-26T18:47:36.348199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntest_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:47:36.352907Z","iopub.execute_input":"2021-08-26T18:47:36.353569Z","iopub.status.idle":"2021-08-26T18:47:40.775301Z","shell.execute_reply.started":"2021-08-26T18:47:36.353521Z","shell.execute_reply":"2021-08-26T18:47:40.774083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Pre_processing(df):\n    \"\"\"\n    This function performs custom normalization on text column\n    \"\"\"\n    #Defining the stop words from nltk\n    stopwords_all = stopwords.words('english')\n    lemmatizer = WordNetLemmatizer()\n    df=df.apply(lambda x: re.sub(r'\\([^()]*\\)', '', x)) #Remove paranthesis\n    df=df.apply(lambda x: x.lower())  #Lowercase the reviews\n    df=df.apply(lambda x: re.sub('\\w*\\d\\w*','', x)) #Remove digits and words containing digits\n    df=df.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x)) #Remove Punctuations\n    df=df.apply(lambda x: re.sub(' +',' ',x))  # remove extra spaces\n    df=df.apply(lambda x: re.sub(\"</?.*?>\",\" <> \",x))  #remove tags\n    df=df.apply(lambda x: \" \".join(lemmatizer.lemmatize(word) for word in x.split())) # lemmatization\n    #df=df.apply(lambda x: \" \".join([word for word in x.split() if len(word)>1])) # remove single characters like a \n    df=df.apply(lambda x: \" \".join([word for word in x.split() if word not in stopwords_all]))\n    df=df.apply(lambda x: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',x))\n    \n    symbols_to_isolate = '.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊɒ∞§{}·τα❤☺ɡ|¢→̶`❥━┣┫┗Ｏ►★©―ɪ✔®\\x96\\x92●£♥➤´¹☕≈÷♡◐║▬′ɔː€۩۞†μ✒➥═☆ˌ◄½ʻπδηλσερνʃ✬ＳＵＰＥＲＩＴ☻±♍µº¾✓◾؟．⬅℅»Вав❣⋅¿¬♫ＣＭβ█▓▒░⇒⭐›¡₂₃❧▰▔◞▀▂▃▄▅▆▇↙γ̄″☹➡«φ⅓„✋：¥̲̅́∙‛◇✏▷❓❗¶˚˙）сиʿ✨。ɑ\\x80◕！％¯−ﬂﬁ₁²ʌ¼⁴⁄₄⌠♭✘╪▶☭✭♪☔☠♂☃☎✈✌✰❆☙○‣⚓年∎ℒ▪▙☏⅛ｃａｓǀ℮¸ｗ‚∼‖ℳ❄←☼⋆ʒ⊂、⅔¨͡๏⚾⚽Φ×θ￦？（℃⏩☮⚠月✊❌⭕▸■⇌☐☑⚡☄ǫ╭∩╮，例＞ʕɐ̣Δ₀✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒↑☝ɹ✅☛♩☞ＡＪＢ◔◡↓♀⬆̱ℏ\\x91⠀ˤ╚↺⇤∏✾◦♬³の｜／∵∴√Ω¤☜▲↳▫‿⬇✧ｏｖｍ－２０８＇‰≤∕ˆ⚜☁'\n    symbols_to_delete = '\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xad😢🐶️\\uf0e0😜😎👊\\u200b\\u200e😁عدويهصقأناخلىبمغر😍💖💵Е👎😀😂\\u202a\\u202c🔥😄🏻💥ᴍʏʀᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ😋👏שלוםבי😱‼\\x81エンジ故障\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘אעכח💩💯⛽🚄🏼ஜ😖ᴠ🚲‐😟😈💪🙏🎯🌹😇💔😡\\x7f👌ἐὶήιὲκἀίῃἴξ🙄Ｈ😠\\ufeff\\u2028😉😤⛺🙂\\u3000تحكسة👮💙فزط😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷улкнПоАН🐾🐕😆ה🔗🚽歌舞伎🙈😴🏿🤗🇺🇸мυтѕ⤵🏆🎃😩\\u200a🌠🐟💫💰💎эпрд\\x95🖐🙅⛲🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢᵒʳʸᴼᴷᴺʷᵗʰᵉᵘ\\x13🚬🤓\\ue602😵άοόςέὸתמדףנרךצט😒͝🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10成都😣⏺😌🤑🌏😯ех😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613小土豆🏡❔⁉\\u202f👠》कर्मा🇹🇼🌸蔡英文🌞🎲レクサス😛外国人关系Сб💋💀🎄💜🤢َِьыгя不是\\x9c\\x9d🗑\\u2005💃📣👿༼つ༽😰ḷЗз▱ц￼🤣卖温哥华议会下降你失去所有的钱加拿大坏税骗子🐝ツ🎅\\x85🍺آإشء🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003🚀🤴ʲшчИОРФДЯМюж😝🖑ὐύύ特殊作戦群щ💨圆明园קℐ🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪天一家⚲\\u2006⚭⚆⬭⬯⏖新✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ϖ\\u2000үսᴦᎥһͺ\\u2007հ\\u2001ɩｙｅ൦ｌƽｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓ё𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘ͦ💸ج패티Ｗ𝙇ᵻ👂👃ɜ🎫\\uf0a7БУі🚢🚂ગુજરાતીῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼☘﴾̯﴿₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸公寓养宠物吗🏄🐀🚑🤷操美𝒑𝒚𝒐𝑴🤙🐒欢迎来到阿拉斯ספ𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ🦄巨收赢得白鬼愤怒要买额ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊多伦🐽🎻🎹⛓🏹🍷🦆为和中友谊祝贺与其想象对法如直接问用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용🔫👁凸ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＮＧ👣\\uf020っ🏉ф💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی𝑮𝗕𝗴🍒ꜥⲣⲏ🐑⏰鉄リ事件ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シ虚偽屁理屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛ⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫کΒώ💢ΜΟΝΑΕ🇱♲𝝈↴💒⊘Ȼ🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷ｇｒｎｔｉｄｕｆｂｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10aლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'\n\n    isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n    remove_dict = {ord(c):f'' for c in symbols_to_delete}\n    \n    df = df.apply(lambda x: x.translate(remove_dict)) \n    df = df.apply(lambda x: x.translate(isolate_dict))\n    \n    return df\n\ntrain_df['comment_text'] = Pre_processing(train_df['comment_text'])","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:47:40.7778Z","iopub.execute_input":"2021-08-26T18:47:40.778247Z","iopub.status.idle":"2021-08-26T18:49:19.65861Z","shell.execute_reply.started":"2021-08-26T18:47:40.778196Z","shell.execute_reply":"2021-08-26T18:49:19.65731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_final = train_df[:int(len(train_df)*0.85*0.8)]\n# validation_final = train_df[int(len(train_df)*0.85*0.8): int(len(train_df)*0.85) ]\n# test_final = train_df[int(len(train_df)*0.85) :  ]\n\n# y_train = train_final[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n# y_val = validation_final[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n# y_test =  test_final[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n\n# target_col = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:49:19.664295Z","iopub.execute_input":"2021-08-26T18:49:19.666757Z","iopub.status.idle":"2021-08-26T18:49:19.67417Z","shell.execute_reply.started":"2021-08-26T18:49:19.666701Z","shell.execute_reply":"2021-08-26T18:49:19.672554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_col = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n\ntrain_df= train_df.append(train_df[train_df.threat==1], ignore_index=True)\ntrain_df= train_df.append(train_df[train_df.threat==1], ignore_index=True)\ntrain_df= train_df.append(train_df[train_df.identity_hate==1], ignore_index=True)\ntrain_df= train_df.append(train_df[train_df.severe_toxic==1], ignore_index=True)\n\nx_tr, X_test, y_tr, y_test = train_test_split(train_df['comment_text'], train_df[target_col], test_size=0.15, random_state=32,stratify=train_df[target_col])\nx_train, x_val, y_train, y_val = train_test_split(x_tr, y_tr, test_size=0.193367882, random_state=42,stratify=y_tr)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:49:19.679777Z","iopub.execute_input":"2021-08-26T18:49:19.683388Z","iopub.status.idle":"2021-08-26T18:49:23.769247Z","shell.execute_reply.started":"2021-08-26T18:49:19.683342Z","shell.execute_reply":"2021-08-26T18:49:23.768285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_tokens = 53194\nmaxlen=100\n\ntok = Tokenizer(num_words=max_tokens, oov_token='UNK')\ntok.fit_on_texts(x_train)\ntok.num_words=max_tokens\nx_train_seq = tok.texts_to_sequences(x_train)\nx_train_padded = pad_sequences(x_train_seq, maxlen=maxlen)\n\nx_val_seq = tok.texts_to_sequences(x_val)\nx_val_padded = pad_sequences(x_val_seq, maxlen=maxlen)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:49:23.771257Z","iopub.execute_input":"2021-08-26T18:49:23.77186Z","iopub.status.idle":"2021-08-26T18:49:35.48157Z","shell.execute_reply.started":"2021-08-26T18:49:23.771804Z","shell.execute_reply":"2021-08-26T18:49:35.480431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_tokens, 64, input_length=maxlen))\nmodel.add(Conv1D(filters=256,kernel_size=7,padding=\"same\"))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:49:35.484585Z","iopub.execute_input":"2021-08-26T18:49:35.484865Z","iopub.status.idle":"2021-08-26T18:49:38.142392Z","shell.execute_reply.started":"2021-08-26T18:49:35.484836Z","shell.execute_reply":"2021-08-26T18:49:38.141345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, to_file= 'model.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:49:38.145994Z","iopub.execute_input":"2021-08-26T18:49:38.146306Z","iopub.status.idle":"2021-08-26T18:49:38.71056Z","shell.execute_reply.started":"2021-08-26T18:49:38.146277Z","shell.execute_reply":"2021-08-26T18:49:38.708969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n# Compile Model\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=[\"accuracy\"])\n\nmy_callbacks = [\n    EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n    ModelCheckpoint(filepath='cnn_{epoch:02d}_{val_accuracy:.04f}.h5',\n                    save_best_only=True),\n]\n\ny_train= np.array(y_train)\ny_val= np.array(y_val)\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(x_train_padded, y_train,batch_size=256,callbacks=my_callbacks, epochs=10,validation_data=(x_val_padded, y_val))\n\nmodel.save('/output/LSTM_Glove_19082021.h5')","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:49:38.712319Z","iopub.execute_input":"2021-08-26T18:49:38.712641Z","iopub.status.idle":"2021-08-26T18:51:34.640633Z","shell.execute_reply.started":"2021-08-26T18:49:38.71261Z","shell.execute_reply":"2021-08-26T18:51:34.639425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny_val_predicted = model.predict(x_val_padded)  # predict_proba\nresult=np.ndarray(shape=(6,4))\nfor i,col in enumerate(target_col):\n    \n    y_val_predicted[:,i]= [ 1 if i>= 0.5 else 0 for i in y_val_predicted[:,i]]\n\n    print(classification_report(y_val_predicted[:,i], y_val[:,i]))\n\n    print('\\nConfusion matrix\\n',confusion_matrix(y_val_predicted[:,i], y_val[:,i]))\n    print(classification_report(y_val_predicted[:,i], y_val[:,i]))\n\n    accuracy = accuracy_score(y_val_predicted[:,i], y_val[:,i])\n    print('Accuracy: %f' % accuracy)\n\n    precision = precision_score(y_val_predicted[:,i], y_val[:,i])\n    print('Precision: %f' % precision)\n\n    # recall: tp / (tp + fn)\n    recall = recall_score(y_val_predicted[:,i], y_val[:,i])\n    print('Recall: %f' % recall)\n\n    # f1: 2 tp / (2 tp + fp + fn)\n    f1 = f1_score(y_val_predicted[:,i], y_val[:,i])\n    print('F1 score: %f' % f1)\n    result[i][0]=accuracy\n    result[i][1]=precision\n    result[i][2]=recall\n    result[i][3]=f1  \n    \n       \n    plot_confusion_matrix(cm       = confusion_matrix(y_val_predicted[:,i], y_val[:,i]), \n                      normalize    = False,\n                      target_names = ['Not {}'.format(col) , '{}'.format(col)],\n                      title        = \"Confusion Matrix\")\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:51:34.64249Z","iopub.execute_input":"2021-08-26T18:51:34.643123Z","iopub.status.idle":"2021-08-26T18:51:40.709036Z","shell.execute_reply.started":"2021-08-26T18:51:34.643079Z","shell.execute_reply":"2021-08-26T18:51:40.707496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = dict()\nf = open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:51:40.710661Z","iopub.execute_input":"2021-08-26T18:51:40.711095Z","iopub.status.idle":"2021-08-26T18:51:59.154097Z","shell.execute_reply.started":"2021-08-26T18:51:40.711065Z","shell.execute_reply":"2021-08-26T18:51:59.152945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((max_tokens, 100))\n\nfor word, i in tok.word_index.items():\n    if i >= max_tokens: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:51:59.155934Z","iopub.execute_input":"2021-08-26T18:51:59.15643Z","iopub.status.idle":"2021-08-26T18:51:59.30806Z","shell.execute_reply.started":"2021-08-26T18:51:59.156397Z","shell.execute_reply":"2021-08-26T18:51:59.306886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(Embedding(max_tokens, 100, input_length=maxlen, weights=[embedding_matrix],trainable=False ))\nmodel2.add(Conv1D(filters=256,kernel_size=7,padding=\"same\"))\nmodel2.add(GlobalMaxPooling1D())\nmodel2.add(Dense(32,activation='relu'))\nmodel2.add(Dropout(0.1))\nmodel2.add(Dense(6, activation='sigmoid'))\nmodel2.summary()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:51:59.309753Z","iopub.execute_input":"2021-08-26T18:51:59.310466Z","iopub.status.idle":"2021-08-26T18:51:59.449269Z","shell.execute_reply.started":"2021-08-26T18:51:59.31042Z","shell.execute_reply":"2021-08-26T18:51:59.448083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(x_train_padded, y_train,batch_size=256,callbacks=my_callbacks, epochs=10,validation_data=(x_val_padded, y_val))\n\nmodel2.save('/output/cnn_Glove_19082021.h5')","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:51:59.450968Z","iopub.execute_input":"2021-08-26T18:51:59.451519Z","iopub.status.idle":"2021-08-26T18:53:03.287309Z","shell.execute_reply.started":"2021-08-26T18:51:59.451459Z","shell.execute_reply":"2021-08-26T18:53:03.286018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny_val_predicted = model2.predict(x_val_padded)  # predict_proba\nresult=np.ndarray(shape=(6,4))\nfor i,col in enumerate(target_col):\n    \n    y_val_predicted[:,i]= [ 1 if i>= 0.5 else 0 for i in y_val_predicted[:,i]]\n\n    print(classification_report(y_val_predicted[:,i], y_val[:,i]))\n\n    print('\\nConfusion matrix\\n',confusion_matrix(y_val_predicted[:,i], y_val[:,i]))\n    print(classification_report(y_val_predicted[:,i], y_val[:,i]))\n\n    accuracy = accuracy_score(y_val_predicted[:,i], y_val[:,i])\n    print('Accuracy: %f' % accuracy)\n\n    precision = precision_score(y_val_predicted[:,i], y_val[:,i])\n    print('Precision: %f' % precision)\n\n    # recall: tp / (tp + fn)\n    recall = recall_score(y_val_predicted[:,i], y_val[:,i])\n    print('Recall: %f' % recall)\n\n    # f1: 2 tp / (2 tp + fp + fn)\n    f1 = f1_score(y_val_predicted[:,i], y_val[:,i])\n    print('F1 score: %f' % f1)\n    result[i][0]=accuracy\n    result[i][1]=precision\n    result[i][2]=recall\n    result[i][3]=f1  \n    \n       \n    plot_confusion_matrix(cm       = confusion_matrix(y_val_predicted[:,i], y_val[:,i]), \n                      normalize    = False,\n                      target_names = ['Not {}'.format(col) , '{}'.format(col)],\n                      title        = \"Confusion Matrix\")\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2021-08-26T18:53:03.28876Z","iopub.execute_input":"2021-08-26T18:53:03.289155Z","iopub.status.idle":"2021-08-26T18:53:10.063597Z","shell.execute_reply.started":"2021-08-26T18:53:03.289124Z","shell.execute_reply":"2021-08-26T18:53:10.061929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}